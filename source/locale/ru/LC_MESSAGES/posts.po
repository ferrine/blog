# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Max Kochurov
# This file is distributed under the same license as the In Search of the Holy Posterior package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# Daria Galimzianova, 2023
# Maxim Kochurov, 2023
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: In Search of the Holy Posterior\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-07-02 13:29+0000\n"
"PO-Revision-Date: 2023-05-01 17:42+0000\n"
"Last-Translator: Maxim Kochurov, 2023\n"
"Language-Team: Russian (https://app.transifex.com/ferrine/teams/167491/ru/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: ru\n"
"Plural-Forms: nplurals=4; plural=(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<12 || n%100>14) ? 1 : n%10==0 || (n%10>=5 && n%10<=9) || (n%100>=11 && n%100<=14)? 2 : 3);\n"

#: ../../source/posts/2022/loo-pit/post.ipynb:9
msgid "Interpreting LOO-PIT"
msgstr "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è LOO-PIT"

#: ../../source/posts/2022/loo-pit/post.ipynb-1
msgid "loo-pit"
msgstr "loo-pit"

#: ../../source/posts/2022/loo-pit/post.ipynb:43
msgid ""
"I got a question about interpreting plots you get from `Arviz "
"<https://arviz-devs.github.io/>`__. This particular one was about LOO-PIT, "
"which you can find `here <https://arviz-"
"devs.github.io/arviz/api/generated/arviz.plot_loo_pit.html>`__."
msgstr ""
"–ú–Ω–µ –∑–∞–¥–∞–ª–∏ –≤–æ–ø—Ä–æ—Å –æ–± –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Å `Arviz "
"<https://arviz-devs.github.io/>. `__. –ò–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –±—ã–ª –ø—Ä–æ LOO-PIT, –∫–æ—Ç–æ—Ä—ã–π "
"–º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ `–∑–¥–µ—Å—å <https://arviz-"
"devs.github.io/arviz/api/generated/arviz.plot_loo_pit.html> `__."

#: ../../source/posts/2022/loo-pit/post.ipynb:45
msgid ""
"LOO-PIT is a useful concept to debug your model after inference is done. You"
" can see if you have outliers and figure them out. By taking in account the "
"feedback, you can later improve your model with better error model."
msgstr ""
"LOO-PIT ‚Äî –ø–æ–ª–µ–∑–Ω–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ —Å–¥–µ–ª–∞–Ω"
" –≤—ã–≤–æ–¥. –í—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å, –µ—Å—Ç—å –ª–∏ —É –≤–∞—Å –≤—ã–±—Ä–æ—Å—ã, –∏ –≤—ã—è—Å–Ω–∏—Ç—å, —á—Ç–æ –æ–Ω–∏ –∏–∑ "
"—Å–µ–±—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç. –ü—Ä–∏–Ω–∏–º–∞—è –≤–æ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å, –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–∑–∂–µ "
"—É–ª—É—á—à–∏—Ç—å —Å–≤–æ—é –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –æ—à–∏–±–æ–∫."

#: ../../source/posts/2022/loo-pit/post.ipynb:47
msgid ""
"Let‚Äôs go into more details about why you ever need this plot, how to work "
"with it, and which actions to take after you see it. A case-study."
msgstr ""
"–î–∞–≤–∞–π—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ–≥–æ–≤–æ—Ä–∏–º –æ —Ç–æ–º, –∑–∞—á–µ–º –≤–∞–º –≤–æ–æ–±—â–µ –Ω—É–∂–µ–Ω —ç—Ç–æ—Ç –≥—Ä–∞—Ñ–∏–∫, –∫–∞–∫ —Å"
" –Ω–∏–º —Ä–∞–±–æ—Ç–∞—Ç—å –∏ –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—å –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã –µ–≥–æ —É–≤–∏–¥–∏—Ç–µ. "
"–£—á–∏–º—Å—è –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ."

#: ../../source/posts/2022/loo-pit/post.ipynb:59
msgid "Integral transform"
msgstr "–ò–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"

#: ../../source/posts/2022/loo-pit/post.ipynb:70
msgid ""
"First, we need a model to demonstrate the pathologies that can be diagnosed "
"with the LOO-PIT. I will use linear regression with robust likelihood to "
"show all possible cases that can be fixed just by looking at the plot."
msgstr ""
"–í–æ-–ø–µ—Ä–≤—ã—Ö, –Ω–∞–º –Ω—É–∂–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–∞—Ç–æ–ª–æ–≥–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ "
"–¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–º–æ—â—å—é LOO-PIT. –Ø –±—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é —Å "
"—É—Å—Ç–æ–π—á–∏–≤—ã–º –∫ –≤—ã–±—Ä–æ—Å–∞–º –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ–º, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–ª—É—á–∞–∏, "
"–∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –ø—Ä–æ—Å—Ç–æ –≤–∑–≥–ª—è–Ω—É–≤ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫."

#: ../../source/posts/2022/loo-pit/post.ipynb:81
msgid ""
"But wait, what is LOO-PIT you ask? Ok, let's give some theory before we "
"start."
msgstr ""
"–ù–æ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, —á—Ç–æ —Ç–∞–∫–æ–µ LOO-PIT, —Å–ø—Ä–æ—Å–∏—Ç–µ –≤—ã? –•–æ—Ä–æ—à–æ, –¥–∞–≤–∞–π—Ç–µ –Ω–µ–º–Ω–æ–≥–æ "
"—Ç–µ–æ—Ä–∏–∏, –ø—Ä–µ–∂–¥–µ —á–µ–º –º—ã –Ω–∞—á–Ω–µ–º."

#: ../../source/posts/2022/loo-pit/post.ipynb:83
msgid ""
"LOO stands for Leave One Out validation. We usually want LOO to cross-"
"validate our model. We train it on all but the one observation and then make"
" predictions on the one that was the holdout. The same for every single "
"observation."
msgstr ""
"LOO —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ –≤–∞–ª–∏–¥–∞—Ü–∏—è Leave One Out. –û–±—ã—á–Ω–æ –º—ã —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã LOO"
" –≤—ã–ø–æ–ª–Ω—è–ª–∞ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏. –ú—ã –æ–±—É—á–∞–µ–º –µ–µ –Ω–∞ –≤—Å–µ—Ö "
"–Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö, –∫—Ä–æ–º–µ –æ–¥–Ω–æ–≥–æ, –∞ –∑–∞—Ç–µ–º –¥–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —É–¥–µ—Ä–∂–∞–Ω–Ω–æ–≥–æ "
"–Ω–∞–±–ª—é–¥–µ–Ω–∏—è. –ü–æ–≤—Ç–æ—Ä—è–µ–º —Ç–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è."

#: ../../source/posts/2022/loo-pit/post.ipynb:84
msgid ""
"PIT stands for Probability Integral Transformation. Briefly, it transforms "
"any continuous distribution into a Uniform distribution."
msgstr ""
"PIT –æ–∑–Ω–∞—á–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏. –í–∫—Ä–∞—Ç—Ü–µ, –æ–Ω–æ "
"–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª—é–±–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ."

#: ../../source/posts/2022/loo-pit/post.ipynb:86
msgid ""
"While it is relatively clear what is LOO, it is not super clear, what is "
"PIT. Let's make some visualizations."
msgstr ""
"–•–æ—Ç—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —è—Å–Ω–æ, —á—Ç–æ —Ç–∞–∫–æ–µ LOO, –Ω–µ –æ—á–µ–Ω—å —è—Å–Ω–æ, —á—Ç–æ —Ç–∞–∫–æ–µ PIT. –î–∞–≤–∞–π—Ç–µ"
" —Å–¥–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π."

#: ../../source/posts/2022/loo-pit/post.ipynb:107
msgid ""
"\\begin{align}\n"
"t &\\sim \\operatorname{StudentT}(0, 1)\\\\\n"
"u &= \\operatorname{StudentT}(0, 1)\\operatorname{.cdf}(t)\\\\\n"
"u &\\sim \\operatorname{Uniform}[0, 1]\n"
"\\end{align}"
msgstr ""
"\\begin{align}\n"
"t &\\sim \\operatorname{StudentT}(0, 1)\\\\\n"
"u &= \\operatorname{StudentT}(0, 1)\\operatorname{.cdf}(t)\\\\\n"
"u &\\sim \\operatorname{Uniform}[0, 1]\n"
"\\end{align}"

#: ../../source/posts/2022/loo-pit/post.ipynb:153
msgid ""
"We see that the cumulative transform does the job and transforms our "
"StudentT random variables to the Uniform distribution. Similar ideas are "
"used for LOO-PIT."
msgstr ""
"–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤–æ—é —Ä–∞–±–æ—Ç—É –∏ "
"–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞—à–∏ —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã StudentT –≤ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ. "
"–ê–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –∏–¥–µ–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è LOO-PIT."

#: ../../source/posts/2022/loo-pit/post.ipynb:155
msgid ""
"`Compute <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/stats/stats.py#L1674>`__"
" pointwise likelihood for all the observations"
msgstr ""
"`–í—ã—á–∏—Å–ª–∏—Ç—å <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/stats/stats.py#L1674>"
" `__ —Ç–æ—á–µ—á–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≤—Å–µ—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π"

#: ../../source/posts/2022/loo-pit/post.ipynb:156
msgid ""
"`Calculate <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/stats/stats.py#L770>`__"
" LOO likelihood approximation using `pareto smoothed importance sampling "
"<https://arxiv.org/abs/1507.02646>`__"
msgstr ""
"`–†–∞—Å—Å—á–∏—Ç–∞—Ç—å <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/stats/stats.py#L770>"
" `__ LOO –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `—Å–≥–ª–∞–∂–µ–Ω–Ω–æ–π –ø–æ –ü–∞—Ä–µ—Ç–æ "
"–≤—ã–±–æ—Ä–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ <https://arxiv.org/abs/1507.02646> `__"

#: ../../source/posts/2022/loo-pit/post.ipynb:157
msgid ""
"`Approximate <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/stats/stats.py#L1729>`__"
" ECDF (Empirical CDF) values of the posterior predictive (for each sample)"
msgstr ""
"`–ê–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/stats/stats.py#L1729>"
" `__ –≤—ã–±–æ—Ä–æ—á–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (Empirical CDF) –∑–Ω–∞—á–µ–Ω–∏—è "
"–∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ (–¥–ª—è –∫–∞–∂–¥–æ–π –≤—ã–±–æ—Ä–∫–∏)"

#: ../../source/posts/2022/loo-pit/post.ipynb:158
msgid ""
"`Approximate <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/plots/loopitplot.py#L177>`__"
" the ECDF function for visualization with KDE"
msgstr ""
"`–ê–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å <https://github.com/arviz-"
"devs/arviz/blob/9d0746972c9bcb1a3063c0ca00ff08a0b0c093ee/arviz/plots/loopitplot.py#L177>"
" `__ —Ñ—É–Ω–∫—Ü–∏—é ECDF –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ KDE"

#: ../../source/posts/2022/loo-pit/post.ipynb:160
msgid ""
"If the likelihood is well specified, we'll see a uniform distribution of "
"ECDF values, means, uniform KDE. Just like on the plot above"
msgstr ""
"–ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ö–æ—Ä–æ—à–æ –∑–∞–¥–∞–Ω–∞, –º—ã —É–≤–∏–¥–∏–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π"
" ECDF, —Ç. –µ. —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π KDE. –¢–∞–∫ –∂–µ, –∫–∞–∫ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ –≤—ã—à–µ"

#: ../../source/posts/2022/loo-pit/post.ipynb:172
msgid "Case study"
msgstr "–†–∞–±–æ—Ç–∞–µ–º —Å –ø—Ä–∏–º–µ—Ä–æ–º"

#: ../../source/posts/2022/loo-pit/post.ipynb:175
msgid "Generative model"
msgstr "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å"

#: ../../source/posts/2022/loo-pit/post.ipynb:177
msgid ""
"Our data will be generated with student T likelihood. We can adjust the "
"degrees of freedom in the model to take care of the noise magnitude. This "
"example will also be a very good way to visualize pathologies"
msgstr ""
"–ù–∞—à–∏ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Å –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ–º –°—Ç—å—é–¥–µ–Ω—Ç–∞. –ú—ã –º–æ–∂–µ–º "
"–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å—Ç–µ–ø–µ–Ω–∏ —Å–≤–æ–±–æ–¥—ã –≤ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø–æ–∑–∞–±–æ—Ç–∏—Ç—å—Å—è –æ –≤–µ–ª–∏—á–∏–Ω–µ —à—É–º–∞. –≠—Ç–æ—Ç"
" –ø—Ä–∏–º–µ—Ä —Ç–∞–∫–∂–µ –±—É–¥–µ—Ç –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∏–º —Å–ø–æ—Å–æ–±–æ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞—Ç–æ–ª–æ–≥–∏–π."

#: ../../source/posts/2022/loo-pit/post.ipynb:219
msgid "Generating example data"
msgstr "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"

#: ../../source/posts/2022/loo-pit/post.ipynb:221
msgid ""
"I do not think our data or model should be complicated. The whole point of "
"the LOO-PIT is in defining likelihood, rarely model structure or "
"flexibility. We can easily illustrate this with a univariate linear "
"regression."
msgstr ""
"–Ø –Ω–µ –¥—É–º–∞—é, —á—Ç–æ –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ª–æ–∂–Ω—ã–º–∏. –í–µ—Å—å —Å–º—ã—Å–ª LOO-"
"PIT –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏. –û–Ω–∞ —Ä–µ–¥–∫–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É "
"–∏–ª–∏ –≥–∏–±–∫–æ—Å—Ç—å. –ú—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–π "
"–ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏."

#: ../../source/posts/2022/loo-pit/post.ipynb:261
msgid "Bayesian detective"
msgstr "–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –¥–µ—Ç–µ–∫—Ç–∏–≤"

#: ../../source/posts/2022/loo-pit/post.ipynb:264
msgid "Normal likelihood"
msgstr "–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ"

#: ../../source/posts/2022/loo-pit/post.ipynb:266
msgid ""
"Most of the models start simple. The simple model in our case starts with a "
"Normal likelihood."
msgstr ""
"–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –ø—Ä–æ—Å—Ç–æ–≥–æ. –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ "
"–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è."

#: ../../source/posts/2022/loo-pit/post.ipynb:569
#: ../../source/posts/2022/loo-pit/post.ipynb:1038
msgid "Checks"
msgstr "–ü—Ä–æ–≤–µ—Ä–∫–∏ "

#: ../../source/posts/2022/loo-pit/post.ipynb:571
msgid ""
"You know nothing about the underlying true model, so I'll keep the intrigue "
"about posterior vs true values to the end."
msgstr ""
"–í—ã –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞–µ—Ç–µ –æ –ª–µ–∂–∞—â–µ–π –≤ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–∏–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –ø–æ—ç—Ç–æ–º—É —è —Å–æ—Ö—Ä–∞–Ω—é "
"–∏–Ω—Ç—Ä–∏–≥—É –æ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω—ã—Ö –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –¥–æ –∫–æ–Ω—Ü–∞."

#: ../../source/posts/2022/loo-pit/post.ipynb:573
msgid "Let's look at LOO-PIT"
msgstr "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ LOO-PIT"

#: ../../source/posts/2022/loo-pit/post.ipynb:602
msgid ""
"Disclaimer. I write these lines as I would first see it, without conducting "
"further analysis. This gives you a good thought process happening in my "
"head."
msgstr ""
"–û—Ç–∫–∞–∑ –æ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏. –Ø –ø–∏—à—É —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–∫, –∫–∞–∫ —É–≤–∏–¥–µ–ª –±—ã –∏—Ö —Å–Ω–∞—á–∞–ª–∞, "
"–Ω–µ –ø—Ä–æ–≤–æ–¥—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–º —Ö–æ—Ä–æ—à–∏–π –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π "
"–ø—Ä–æ—Ü–µ—Å—Å, –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–∏–π –≤ –º–æ–µ–π –≥–æ–ª–æ–≤–µ."

#: ../../source/posts/2022/loo-pit/post.ipynb:604
msgid ""
"Oh, you might think! Something is going wrong. What do we read from the "
"plot? Let's operate with facts: \\* We have to recall that what we plot here"
" is CDF of the posterior applied to the observed data \\* Since we specified"
" normal distribution in likelihood our CDF has similar properties (no "
"outliers) \\* Looks like our data is mostly concentrated inside the high-"
"density region and not represented on the tails"
msgstr ""
"–û, –º–æ–∂–Ω–æ –ø–æ–¥—É–º–∞—Ç—å! –ß—Ç–æ-—Ç–æ –∏–¥–µ—Ç –Ω–µ —Ç–∞–∫. –ß—Ç–æ –º—ã —á–∏—Ç–∞–µ–º –∏–∑ –≥—Ä–∞—Ñ–∏–∫–∞? –î–∞–≤–∞–π—Ç–µ "
"–æ–ø–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–∫—Ç–∞–º–∏: \\* –ú—ã –¥–æ–ª–∂–Ω—ã –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ —Ç–æ, —á—Ç–æ –º—ã —Å—Ç—Ä–æ–∏–º –∑–¥–µ—Å—å, "
"—è–≤–ª—è–µ—Ç—Å—è CDF –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω–æ–≥–æ –∫ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–º –¥–∞–Ω–Ω—ã–º"
" \\* –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —É–∫–∞–∑–∞–ª–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—é, –Ω–∞—à–∞ "
"CDF –∏–º–µ–µ—Ç –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ (–±–µ–∑ –≤—ã–±—Ä–æ—Å–æ–≤) \\* –ü–æ—Ö–æ–∂–µ, –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ –≤ "
"–æ—Å–Ω–æ–≤–Ω–æ–º —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –≤–Ω—É—Ç—Ä–∏ –æ–±–ª–∞—Å—Ç–∏ –≤—ã—Å–æ–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–∞"
" —Ö–≤–æ—Å—Ç–∞—Ö"

#: ../../source/posts/2022/loo-pit/post.ipynb:606
msgid ""
"*I know how I generated my data and, actually, expected a bit different "
"picture. But let's see how it goes*"
msgstr ""
"*–Ø –∑–Ω–∞—é, –∫–∞–∫ —è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ –∏, –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ, –æ–∂–∏–¥–∞–ª –Ω–µ–º–Ω–æ–≥–æ "
"–¥—Ä—É–≥–æ–π –∫–∞—Ä—Ç–∏–Ω—ã. –ù–æ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –ø–æ–π–¥–µ—Ç *"

#: ../../source/posts/2022/loo-pit/post.ipynb:608
msgid "Let's check how does the posterior predictive looks like:"
msgstr "–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑:"

#: ../../source/posts/2022/loo-pit/post.ipynb:637
msgid ""
"Oh, yeah! We get some more intuition from this picture. Data has huge tails,"
" but narrow high-density regions. But for a more complicated distribution, "
"could we have known that we have outliers without the picture? Yes"
msgstr ""
"–ê—Ö, –¥–∞! –ú—ã –ø–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –∏–Ω—Ç—É–∏—Ü–∏–∏ –æ—Ç —ç—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω—ã. –î–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ "
"—Ö–≤–æ—Å—Ç—ã, –Ω–æ —É–∑–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏ –≤—ã—Å–æ–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏. –ù–æ –¥–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ "
"—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–≥–ª–∏ –ª–∏ –º—ã –∑–Ω–∞—Ç—å, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤—ã–±—Ä–æ—Å—ã –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è? –î–∞"

#: ../../source/posts/2022/loo-pit/post.ipynb:723
msgid ""
"With Pareto k diagnostic values we see that we have two outliers. I would "
"not say that two is good. Our example says that we have a very ill-defined "
"model. But only by combining the LOO-PIT check and Pareto k values we can "
"conclude that with symptoms we observe the disease are outliers."
msgstr ""
"–° –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ü–∞—Ä–µ—Ç–æ k –º—ã –≤–∏–¥–∏–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –¥–≤–∞ –≤—ã–±—Ä–æ—Å–∞."
" –Ø –±—ã –Ω–µ —Å–∫–∞–∑–∞–ª, —á—Ç–æ –¥–≤–∞ ‚Äî —ç—Ç–æ —Ö–æ—Ä–æ—à–æ. –ù–∞—à –ø—Ä–∏–º–µ—Ä –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ —É –Ω–∞—Å –æ—á–µ–Ω—å "
"–ø–ª–æ—Ö–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å. –ù–æ —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–¥–∏–Ω–∏–≤ –ø—Ä–æ–≤–µ—Ä–∫—É LOO-PIT –∏ –∑–Ω–∞—á–µ–Ω–∏—è "
"–ü–∞—Ä–µ—Ç–æ k, –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥, —á—Ç–æ –±–æ–ª–µ–∑–Ω—å, —Å–∏–º–ø—Ç–æ–º—ã –∫–æ—Ç–æ—Ä–æ–π –º—ã "
"–Ω–∞–±–ª—é–¥–∞–µ–º, ‚Äî —ç—Ç–æ –≤—ã–±—Ä–æ—Å—ã."

#: ../../source/posts/2022/loo-pit/post.ipynb:725
msgid ""
"But only combining LOO-PIT check and Pareto k values we can conclude that "
"with symptoms we observe the disease are outliers."
msgstr ""
"–ù–æ —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–¥–∏–Ω–∏–≤ –ø—Ä–æ–≤–µ—Ä–∫—É LOO-PIT –∏ –∑–Ω–∞—á–µ–Ω–∏—è –ü–∞—Ä–µ—Ç–æ k, –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å "
"–≤—ã–≤–æ–¥, —á—Ç–æ –±–æ–ª–µ–∑–Ω—å, —Å–∏–º–ø—Ç–æ–º—ã –∫–æ—Ç–æ—Ä–æ–π –º—ã –Ω–∞–±–ª—é–¥–∞–µ–º, ‚Äî —ç—Ç–æ –≤—ã–±—Ä–æ—Å—ã."

#: ../../source/posts/2022/loo-pit/post.ipynb:727
msgid "Could we do more plots? Yes"
msgstr "–ú–æ–∂–µ–º –ª–∏ –º—ã —Å–¥–µ–ª–∞—Ç—å –±–æ–ª—å—à–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤? –î–∞"

#: ../../source/posts/2022/loo-pit/post.ipynb:728
msgid "Would they add more value? Probably"
msgstr "–î–æ–±–∞–≤—è—Ç –ª–∏ –æ–Ω–∏ –±–æ–ª—å—à–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏? –í–µ—Ä–æ—è—Ç–Ω–æ"

#: ../../source/posts/2022/loo-pit/post.ipynb:729
msgid "How much? Not much"
msgstr "–°–∫–æ–ª—å–∫–æ? –ù–µ–º–Ω–æ–≥–æ"

#: ../../source/posts/2022/loo-pit/post.ipynb:741
msgid "Robust likelihood"
msgstr "–†–æ–±–∞—Å—Ç–Ω–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ"

#: ../../source/posts/2022/loo-pit/post.ipynb:743
msgid ""
"Let's go to the second specification of our model. This one will be robust, "
"amazingly robust, we'll try to get as long tails as possible."
msgstr ""
"–ü–µ—Ä–µ–π–¥–µ–º –∫–æ –≤—Ç–æ—Ä–æ–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏. –û–Ω–∞ –±—É–¥–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ–π, "
"—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–∞–¥–µ–∂–Ω–æ–π, –º—ã –ø–æ—Å—Ç–∞—Ä–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ "
"—Ö–≤–æ—Å—Ç—ã."

#: ../../source/posts/2022/loo-pit/post.ipynb:765
msgid "And then use it in the model"
msgstr "–ê –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–µ –≤ –º–æ–¥–µ–ª–∏"

#: ../../source/posts/2022/loo-pit/post.ipynb:1049
msgid "Let's check LOO-PIT again"
msgstr "–î–∞–≤–∞–π—Ç–µ –µ—â–µ —Ä–∞–∑ –ø—Ä–æ–≤–µ—Ä–∏–º LOO-PIT"

#: ../../source/posts/2022/loo-pit/post.ipynb:1081
msgid ""
"Ok, we now see two weird bumps in the LOO-PIT plot. What can we say about "
"this? \\* Remember, these are CDF values of data \\* Yeah, CDF says that in "
"high-density region we have less observations than on tails \\* Let me "
"clarify a tiny bit"
msgstr ""
"–•–æ—Ä–æ—à–æ, —Ç–µ–ø–µ—Ä—å –º—ã –≤–∏–¥–∏–º –¥–≤–µ —Å—Ç—Ä–∞–Ω–Ω—ã–µ –Ω–µ—Ä–æ–≤–Ω–æ—Å—Ç–∏ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ LOO-PIT. –ß—Ç–æ –º—ã "
"–º–æ–∂–µ–º —Å–∫–∞–∑–∞—Ç—å –æ–± —ç—Ç–æ–º? \\* –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è CDF –¥–∞–Ω–Ω—ã—Ö \\* –î–∞, CDF "
"–≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –≤ –æ–±–ª–∞—Å—Ç–∏ —Å –≤—ã—Å–æ–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é —É –Ω–∞—Å –º–µ–Ω—å—à–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, —á–µ–º –Ω–∞ "
"—Ö–≤–æ—Å—Ç–∞—Ö \\* –ü–æ–∑–≤–æ–ª—å—Ç–µ –º–Ω–µ –Ω–µ–º–Ω–æ–≥–æ –ø–æ—è—Å–Ω–∏—Ç—å"

#: ../../source/posts/2022/loo-pit/post.ipynb:1110
msgid ""
"Data seems to be clearly concentrated a bit far from where it should be. We "
"probably underestimated variance - we do not see enough data in the small "
"green region. Let's check the PPC plot to visualize the intuition further."
msgstr ""
"–î–∞–Ω–Ω—ã–µ –∫–∞–∂—É—Ç—Å—è —è–≤–Ω–æ —Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–µ–º–Ω–æ–≥–æ –¥–∞–ª–µ–∫–æ –æ—Ç —Ç–æ–≥–æ –º–µ—Å—Ç–∞, –≥–¥–µ "
"–æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å. –í–µ—Ä–æ—è—Ç–Ω–æ, –º—ã –Ω–µ–¥–æ–æ—Ü–µ–Ω–∏–ª–∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é ‚Äî –º—ã –Ω–µ –≤–∏–¥–∏–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ"
" –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–µ–±–æ–ª—å—à–æ–π –∑–µ–ª–µ–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –î–∞–≤–∞–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏–º –≥—Ä–∞—Ñ–∏–∫ PPC, —á—Ç–æ–±—ã "
"–ª—É—á—à–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç—É–∏—Ü–∏—é."

#: ../../source/posts/2022/loo-pit/post.ipynb:1142
msgid ""
"Oh, what a bad model would you say, outliers are insane in the posterior "
"predictive, we should have less of them. But wait, what about variance, not "
"outliers"
msgstr ""
"–û, –∫–∞–∫–∞—è –ø–ª–æ—Ö–∞—è –º–æ–¥–µ–ª—å, –≤—ã –±—ã —Å–∫–∞–∑–∞–ª–∏, –≤—ã–±—Ä–æ—Å—ã –±–µ–∑—É–º–Ω—ã –≤ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–º "
"–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏, —É –Ω–∞—Å –∏—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–µ–Ω—å—à–µ. –ù–æ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, –∞ –∫–∞–∫ –Ω–∞—Å—á–µ—Ç "
"–¥–∏—Å–ø–µ—Ä—Å–∏–∏, –∞ –Ω–µ –≤—ã–±—Ä–æ—Å–æ–≤?"

#: ../../source/posts/2022/loo-pit/post.ipynb:1175
msgid ""
"Yes, the picture is now more accurate. In our passion to add outliers into "
"the model, we missed the fact that we might actually add way too many of "
"them. Variance also seems to be high, but that seems not a priority this "
"time."
msgstr ""
"–î–∞, –∫–∞—Ä—Ç–∏–Ω–∞ —Å—Ç–∞–ª–∞ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π. –í –Ω–∞—à–µ–π —Å—Ç—Ä–∞—Å—Ç–∏ –∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é –≤—ã–±—Ä–æ—Å–æ–≤ –≤ "
"–º–æ–¥–µ–ª—å –º—ã —É–ø—É—Å—Ç–∏–ª–∏ —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –º—ã –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å –∏—Ö —Å–ª–∏—à–∫–æ–º "
"–º–Ω–æ–≥–æ. –î–∏—Å–ø–µ—Ä—Å–∏—è —Ç–∞–∫–∂–µ –∫–∞–∂–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–π, –Ω–æ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ —ç—Ç–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è "
"–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º."

#: ../../source/posts/2022/loo-pit/post.ipynb:1177
msgid "Take-outs: reduce outliers, variance is not a priority"
msgstr ""
"–í—ã–≤–æ–¥—ã: —É–º–µ–Ω—å—à–∏—Ç–µ –≤—ã–±—Ä–æ—Å—ã, –¥–∏—Å–ø–µ—Ä—Å–∏—è –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –±–æ–ª—å—à–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π —Å–µ–π—á–∞—Å"

#: ../../source/posts/2022/loo-pit/post.ipynb:1189
msgid "Student T likelihood"
msgstr "–ü—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –°—Ç—å—é–¥–µ–Ω—Ç–∞"

#: ../../source/posts/2022/loo-pit/post.ipynb:1191
msgid "**Did you know?**"
msgstr "**–í—ã –∑–Ω–∞–ª–∏?**"

#: ../../source/posts/2022/loo-pit/post.ipynb:1193
msgid ":math:`\\nu` parameter in StudentT controls *\"how heavy are the tails\"*"
msgstr ""
"–ü–∞—Ä–∞–º–µ—Ç—Ä :math:`\\nu` –≤ StudentT —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–º, *\"–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç—è–∂–µ–ª—ã "
"—Ö–≤–æ—Å—Ç—ã\"*"

#: ../../source/posts/2022/loo-pit/post.ipynb:1195
msgid ":math:`\\nu \\to 0` means crazy many outliers"
msgstr ":math:`\\nu \\to 0` –æ–∑–Ω–∞—á–∞–µ—Ç –±–µ–∑—É–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±—Ä–æ—Å–æ–≤"

#: ../../source/posts/2022/loo-pit/post.ipynb:1196
msgid ":math:`\\nu = 1` is Cauchy (See the likelihood above, gotcha)"
msgstr ":math:`\\nu = 1` ‚Äî –ö–æ—à–∏ (—Å–º. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã—à–µ)"

#: ../../source/posts/2022/loo-pit/post.ipynb:1197
msgid ":math:`\\nu \\to \\infty` gives you normal distribution"
msgstr ":math:`\\nu \\to \\infty` –¥–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"

#: ../../source/posts/2022/loo-pit/post.ipynb:1199
msgid ""
"With StudentT distribution we can control both variance and amount of over-"
"dispersion. How do we construct priors for parameters? \\* :math:`\\nu` - we"
" just checked Cauchy distribution and saw that it generates tons of "
"outliers, we need less, for sure. But not too much. We need zero avoiding "
"prior with relatively large tails \\* :math:`\\sigma` - no specific values, "
"we can just keep the old prior"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1201
msgid ""
"My take on :math:`\\nu` parameter. I informally think of it if I would "
"expect :math:`1/\\nu` rate of outliers in my data. That is very aligned with"
" :math:`\\nu \\to \\infty` converging to normal distribution. Though, this "
"is just intuition, not mathematical interpretation. Use it as long as it "
"applies to your case."
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1244
msgid "This prior should work better"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1537
msgid "What one looks much better. Let's compare our models..."
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1549
msgid "Model Comparison"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1591
msgid ""
"We see a very interesting picture. My thinking was in favor to rank the "
"normal model higher than the Cauchy one. But things are, actually, more "
"complicated than initially perceived. Normal distribution did not take into "
"account outliers and suffered hard. Cauchy was more forgiving to outliers, "
"but as we've seen, more forgiving than necessary. Does it have consequences?"
" Probably, let's see"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1603
msgid "Cards revealed"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1605
msgid ""
"This all is not necessary at all unless we want to know something about our "
"model and the system it describes. Our variables of interest are "
":math:`\\beta` and the intercept:"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1634
msgid ""
"We can also compare variance (scale parameters) that our models estimate"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1663
msgid ""
"Hah, most have nothing in common with the true sigma. Only StudentT managed "
"to recover the true sigma."
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1687
msgid ""
"Posterior for :math:`\\nu` is quite uncertain. Low values generate too much "
"noise, so sigma is less important. The uncertainty in :math:`\\nu` creates a"
" lot of troubles in practice, you need to be really certain in your priors "
"to make use of that. Let's have a look at the posterior predictive again"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1719
msgid "We need to zoom in to see the difference"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1749
msgid "But could I?.."
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:1760
msgid "Maybe we could do any better when we know :math:`\\nu`?"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:2101
msgid ""
"Noise parameters are always tricky, there is a trade-off where there "
"something we can infer and what is left behind the scenes. Do not get "
"trapped in estimating every nuance parameter you ever introduce into the "
"model. There will definitely be something you've left behind, put not "
"informed enough priors and so on. > Actually this is exactly the case with "
"sigma in this example. Half normal is not zero avoiding and the true model "
"has :math:`\\sigma=3` that is away from zero."
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:2113
msgid "Conclusions"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:2115
msgid ""
"LOO-PIT is a great plot that helps you to go look what is going on with the "
"model"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:2117
msgid "check tails and outliers"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:2118
msgid "check variance"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:2120
msgid ""
"LOO-PIT combined with PPC plot and LOO-Pointwise can guide to a better "
"likelihood"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:2121
msgid ""
"Sometimes the model gives you a good answer, but still can be improved with "
"more informed priors, more domain knowledge. Nuance parameters are hard, and"
" you never know the right answer, that's fine"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:4
#: ../../source/posts/2023/r2d2m2/post.ipynb:4
msgid ""
"If you have new ideas after reading the notebook, want to discuss it in more"
" details or want to work with me, you can always :doc:`reach me out "
"</contacts>`."
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:0
#: ../../source/posts/2023/r2d2m2/post.ipynb:0
msgid "Resources"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:11
#: ../../source/posts/2023/r2d2m2/post.ipynb:11
msgid "Download the :download:`post.ipynb`"
msgstr ""

#: ../../source/posts/2022/loo-pit/post.ipynb:12
#: ../../source/posts/2023/r2d2m2/post.ipynb:12
msgid "Download the :download:`environment.yml`"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:9
msgid "The R2D2M2 Prior, the Awwwesome Linear Regression"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb-1
msgid "cover"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:43
msgid ""
"There is an awesome paper that puts a simple idea of interpretable linear "
"regression to an absolute. The `Intuitive Joint Priors for Bayesian Linear "
"Multilevel Models: The R2D2M2 prior <https://arxiv.org/abs/2208.07132>`__. "
"The idea resonates with me a lot, since I admire interpretable priors and "
"the ability to describe the model, not an ability to explain."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:45
msgid ""
"Explaining the model, you try to interpret parameters that are already "
"present in the model, and you need to put some human-readable explanation of"
" the meaning. It reminds me of the top-down approach, then you introduce "
"parameters as you need them."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:47
msgid ""
"Describing the model is the opposite. You start with simple statements that "
"you or your peers both understand. It is usually referred to as the bottom "
"up approach, you start with assumptions you understand and complete the "
"model as you go."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:49
msgid ""
"Mastering the latter is the essence of Bayesian modeling. In this sense, the"
" paper is of great interest to me."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:52
msgid "Boston Housing Data"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:54
msgid ""
"As a benchmark dataset, I took the Boston housing dataset. It has the bare "
"minimum to check the model parametrization. I took the part of the linear "
"regression I really need and ignored varying intercept and varying slope. "
"Here is the full model I'm referring to."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:65
msgid ""
"\\begin{equation}\n"
"    \\begin{aligned}\n"
"y_n & \\sim \\mathcal{N} (\\mu_n, \\sigma^2) \\\\\n"
"\\mu_n &= b_{0}+ \\sum_{i=1}^p x_{ni} b_{i}+  \\sum_{g\\in G_0} u_{0 g_{j[n]}  }  + \\sum_{i=1}^p x_{ni} \\left( \\sum_{g \\in G_i} u_{i g_{j[n]}} \\right) \\\\\n"
"b_{0}& \\sim  p(b_{0}) \\\\\n"
"b_i &\\sim \\mathcal{N} \\left(0, \\frac{\\sigma^2}{\\sigma_{x_i}^2}   \\phi_i \\tau^2\\right), \\quad\n"
"    u_{ig_j} \\sim \\mathcal{N} \\left(0, \\frac{\\sigma^2}{\\sigma_{x_i}^2}   \\phi_{ig} \\tau^2\\right) \\\\\n"
"    \\tau^2&= \\frac{R^2}{1-R^2}\\\\\n"
"R^2  &\\sim \\operatorname{Beta}(\\mu_{R^2},\\varphi_{R^2}), \\ \\\n"
"     \\phi \\sim \\operatorname{Dirichlet} (\\alpha), \\ \\\n"
"     \\sigma \\sim p(\\sigma). \\\\\n"
"\\end{aligned}\n"
"\\end{equation}"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:91
msgid "Let's finally get our hands dirty with code!"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:116
msgid ""
"It has to be remarked that I advocate normalizing your input data so it has "
":math:`\\mathbb{E}[X]=0`, :math:`\\mathbb{V}[X]=1`. This will save you a lot"
" of time to figure out the right scale for slope parameters. Normalizing the"
" output variable also makes a lot of sense. You may also notice that it is "
"actually a part of the R2D2M2 linear regression parameterization. A rule of "
"thumb is to take care of it one way or another."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:127
msgid "b_i \\sim \\mathcal{N} \\left(0, \\frac{\\sigma^2}{\\sigma_{x_i}^2}\\right)"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:150
msgid ""
"We deal with positive outcome, prices. I would also expect the variable "
"effect to have elasticity interpretation. To do so, I apply ``log`` "
"transformation on positive valued regressors."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:193
msgid "Non-centered R2D2M2"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:195
msgid ""
"Whenever I see a normal distribution with mean and scale, I tend to use non-"
"centered parameterization. I was shocked to see how different it is from the"
" centered case. But before we go to the working examples, let's see how to "
"make an amazing model miserably fail."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:481
msgid ""
"Yet it has no divergences, the parameter landscape is very complicated."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:483
msgid "Larger phi discourages large coefficients."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:484
msgid "There is also a banana shape relationship"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:485
msgid ""
"Step size should be very different if you are in the center of the banana or"
" on its boundary."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:486
msgid "Total sampling time on my Supermicro is almost 1 hour ü§Ø"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:498
msgid "Centered R2D2M2"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:500
msgid ""
"To my surprise, this model parametrization is 20 times faster than a non-"
"centered variant. Once you download the notebook, you can play around with "
"some parameters below to get the idea of R2D2M2 strong or weak places. Here "
"is the bare minimum of what you should know about the model:"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:502
msgid "You choose R2:"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:504
msgid "In econometrics classes, you've seen what is R2."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:505
msgid "1 - is a perfect data fit"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:506
msgid "0 - total noise and predictors are irrelevant."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:507
msgid "In a Bayesian setup, you do not pick one, you pick possible ones."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:508
msgid ""
"A suggestion is to pick a :math:`95\\%` range :math:`(R^2_{2.5\\%}, "
"R^2_{97.5\\%})`."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:510
msgid "You choose variable importance:"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:512
msgid ""
"There are usually many variables in the regression, one you might assume is "
"more important than another."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:513
msgid ""
":math:`\\alpha_i` is the parameter to look into to express the knowledge."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:514
msgid ""
":math:`\\alpha_i=1` is total unawareness of the importance of the variable."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:515
msgid ""
":math:`\\alpha_i<1` tells the model you tend to think the :math:`i`'th "
"variable is likely to be not significant."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:516
msgid ":math:`\\alpha_i>1` informs your model not to ignore the variable."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:517
msgid ""
"Extremely large or small :math:`\\alpha_i` may cause numerical problems or "
"divergences"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:519
msgid "0.001 was too small"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:520
msgid "200 was too much"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:521
msgid "You can explore better reasonable bounds using this notebook"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:685
msgid "Combining residual and sigma"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:696
msgid ""
"Sampling is much faster but there are divergences. Let's try to combine "
":math:`R^2` and :math:`\\sigma`"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:860
msgid "Yet, there is correlation in sigma and :math:`R^2`"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:897
msgid "What did the job was:"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:899
msgid "Centered parametrization"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:905
msgid "Attaching residual sigma to the model and :math:`R^2` parametrization"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:923
msgid "Sparse solution"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:925
msgid ""
"Now imagine we have some doubtful variables under consideration. We are "
"unsure we should use them, but some of them may be helpful. To create a "
"model under this setting, we need variable importance which are "
":math:`\\alpha` in the model parameterization."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:1043
msgid ""
"The amount of sparsity is incredible. Yet, there are divergences which are "
"an adventure to investigate."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:1045
msgid ""
"As authors claim, there are instabilities with low :math:`\\alpha`, and "
"general recommendation (personal experience) is to exclude non-informative "
"variables and set variable importance :math:`\\alpha` > 1"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:1079
msgid "Conclusion"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:1081
msgid "There are few practical advices to use R2D2M2 prior:"
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:1083
msgid ""
"Use the prior to weight variables that make sense. The sparsity inducing "
"alpha leads to instabilities (which you may ignore)."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:1084
msgid ""
"Not always centered parametrization was great in my applications. I "
"recommend trying both."
msgstr ""

#: ../../source/posts/2023/r2d2m2/post.ipynb:1085
msgid ""
"Coupling the error term sigma into the model leads to fewer instabilities "
"and more interpretable sigma prior."
msgstr ""
